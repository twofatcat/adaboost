{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2-Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yumin cao, a1754926"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-- read file & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-1</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1      2      3       4       5        6        7        8        9   \\\n",
       "0     1  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710   \n",
       "1     1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017   \n",
       "2     1  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790   \n",
       "3     1  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520   \n",
       "4     1  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430   \n",
       "..   ..    ...    ...     ...     ...      ...      ...      ...      ...   \n",
       "564   1  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390  0.13890   \n",
       "565   1  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400  0.09791   \n",
       "566   1  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302   \n",
       "567   1  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.15200   \n",
       "568  -1   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000   \n",
       "\n",
       "         10  ...      22     23      24      25       26       27      28  \\\n",
       "0    0.2419  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
       "1    0.1812  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
       "2    0.2069  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
       "3    0.2597  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
       "4    0.1809  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
       "..      ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "564  0.1726  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
       "565  0.1752  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
       "566  0.1590  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
       "567  0.2397  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
       "568  0.1587  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "         29      30       31  \n",
       "0    0.2654  0.4601  0.11890  \n",
       "1    0.1860  0.2750  0.08902  \n",
       "2    0.2430  0.3613  0.08758  \n",
       "3    0.2575  0.6638  0.17300  \n",
       "4    0.1625  0.2364  0.07678  \n",
       "..      ...     ...      ...  \n",
       "564  0.2216  0.2060  0.07115  \n",
       "565  0.1628  0.2572  0.06637  \n",
       "566  0.1418  0.2218  0.07820  \n",
       "567  0.2650  0.4087  0.12400  \n",
       "568  0.0000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = '/Users/cao.yumin/Desktop/statistic_of_ml/2/data/wdbc_data.csv'\n",
    "df = pd.read_csv(input_path,header=None)\n",
    "df.drop(columns=[0],inplace=True)\n",
    "'''\n",
    "convert the labels into {-1,+1}\n",
    "'''\n",
    "df.replace({1: {'M': 1, 'B': -1}},inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "split train set and test set\n",
    "'''\n",
    "train = df.iloc[0:300,:]\n",
    "train_x = df.iloc[0:300,1:] \n",
    "train_y = df.iloc[0:300,0:1]\n",
    "test = df.iloc[300:,:]\n",
    "test_x = df.iloc[300:,1:]\n",
    "test_y = df.iloc[300:,0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-- class adaboost & class CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Adaboost:\n",
    "    input- training set; weak learner classifier\n",
    "    output- final classifier G(x); train_test error curve with iterations\n",
    "'''\n",
    "class Adaboost:\n",
    "    def __init__(self,train):\n",
    "        self.train = train\n",
    "        self.train_x = train.iloc[:,1:]\n",
    "        self.train_y = train.iloc[:,0:1]\n",
    "    '''\n",
    "    Gini_CART_DT:\n",
    "\n",
    "    Using CART method as decision stump\n",
    "    Using (a_i+a_i+1)/2 as sort point r(threshold), to have x<r and x>r\n",
    "\n",
    "    goal- find the minist gini index\n",
    "    input- updated training set(with weight distribution Dm)\n",
    "    return- all of the gini indexs; the optimal cut point\n",
    "    '''\n",
    "    def Gini_CART_DT(self,input_train,weight_list):\n",
    "        weighted_train = input_train\n",
    "        weighted_train_x = input_train.iloc[:,1:]\n",
    "        weighted_train_y = input_train.iloc[:,0:1]\n",
    "        self.weight_list = weight_list\n",
    "        temp_list = []\n",
    "        for i in range(len(weighted_train_x.columns)):\n",
    "            target_column = weighted_train_x.iloc[:,i:i+1]\n",
    "            target_column_x_y = pd.concat([weighted_train_y,target_column],axis=1)\n",
    "            by_column = target_column_x_y.columns[1] # column name of target_column\n",
    "            new_target_column = target_column_x_y.sort_values(by=by_column, axis=0, ascending = True)\n",
    "            new_target_column_x = new_target_column.iloc[:,1:] # feature value after sorting\n",
    "            new_target_column_y = new_target_column.iloc[:,0:1] # labels after sorting\n",
    "            \n",
    "            new_threshold = [] # 30 thresholds\n",
    "            for k in range(int(new_target_column.shape[0]/10)):\n",
    "                tlist = [0+10*k,9+10*k] # [0,9], [10,19]... threshold interval 10\n",
    "                threshold = new_target_column_x.iloc[tlist[0]:tlist[0]+1,:].values[0]+\\\n",
    "                                           new_target_column_x.iloc[tlist[1]:tlist[1]+1,:].values[0]\n",
    "                new_threshold.append(threshold[0])\n",
    "            threshold_list = []\n",
    "            for t in range(len(new_threshold)):\n",
    "                error_raws_index1= [0]*new_target_column_x.shape[0] # list of index that wrongly classified\n",
    "                error_raws_index2 = [0]*new_target_column_x.shape[0]\n",
    "                error_raws_index = []\n",
    "            \n",
    "                for m in range(len(error_raws_index1)): # x<r +1; x>r -1\n",
    "                    if target_column.iloc[m:m+1,:].values[0][0]<= new_threshold[t]:\n",
    "                        if target_column_x_y.iloc[m:m+1,0:1].values[0][0]==-1:\n",
    "                            error_raws_index1[m]=1\n",
    "                    elif target_column.iloc[m:m+1,:].values[0][0]>new_threshold[t]:\n",
    "                        if target_column_x_y.iloc[m:m+1,0:1].values[0][0]==1:\n",
    "                            error_raws_index1[m]=1\n",
    "\n",
    "                for m in range(len(error_raws_index2)): # x<r -1; x>r +1\n",
    "                    if target_column.iloc[m:m+1,:].values[0][0]<=new_threshold[t]:\n",
    "                        if target_column_x_y.iloc[m:m+1,0:1].values[0][0]==1:\n",
    "                            error_raws_index2[m]=1\n",
    "                    elif target_column.iloc[m:m+1,:].values[0][0]>new_threshold[t]:\n",
    "                        if target_column_x_y.iloc[m:m+1,0:1].values[0][0]==-1:\n",
    "                            error_raws_index2[m]=1\n",
    "\n",
    "                if error_raws_index1.count(1) >= error_raws_index2.count(1):\n",
    "                    error_raws_index = error_raws_index2\n",
    "                    error_raws_index += ['-'] # x<r -1; x>r +1\n",
    "                elif error_raws_index1.count(1) < error_raws_index2.count(1):\n",
    "                    error_raws_index = error_raws_index1\n",
    "                    error_raws_index += ['+'] # x<r +1; x>r -1\n",
    "                    \n",
    "                error = 0\n",
    "                for s in range(len(self.weight_list)):\n",
    "                    error += self.weight_list[s]* error_raws_index[s]\n",
    "                threshold_list.append([error,new_threshold[t],error_raws_index])\n",
    "            new_thresholdlist = sorted(threshold_list)  \n",
    "            \n",
    "            temp_list.append(new_thresholdlist[0]+[int(weighted_train_x.columns[i])])\n",
    "\n",
    "        new_temp_list = sorted(temp_list) # get the minimum gini of all columns\n",
    "\n",
    "        return new_temp_list[0] # [error, threshold, error_raws_index, column_name]\n",
    "\n",
    "    def get_final_classifier(self):\n",
    "        \n",
    "        initial_weight = 1/self.train.shape[0]\n",
    "        weight_list = [initial_weight]*self.train.shape[0]\n",
    "        m = self.Gini_CART_DT(self.train,weight_list)\n",
    "        error, threshold, error_raws_index, column_name,  = \\\n",
    "                                                    m[0],m[1],m[2],m[3]\n",
    "        \n",
    "        final_classifier = [] # recording alpha and the threshold of G(x)\n",
    "        error_list = [] # recording error for plot\n",
    "        column_names = []\n",
    "        pn = [] # recording error_raws_index[-1] for classify x<r +1 or -1\n",
    "        count=1 # count iteration\n",
    "        random_number = np.random.random(self.train.shape[0]) \n",
    "        \n",
    "        while error > 0.005:\n",
    "            print('error:',error)\n",
    "            pn.append(error_raws_index[-1])\n",
    "            alpha = 1/2*math.log((1-error)/error)\n",
    "            final_classifier.append((alpha,threshold))\n",
    "            error_list.append(error)\n",
    "            column_names.append(column_name)\n",
    "            pn.append(error_raws_index[-1])\n",
    "            \n",
    "            Z = 0 # normalization factor Zm\n",
    "            for r in range(len(error_raws_index[:-1])):\n",
    "                if error_raws_index[r]==0:\n",
    "                    p = weight_list[r]*math.exp(-alpha)\n",
    "                    Z += p\n",
    "                elif error_raws_index[r]==1:\n",
    "                    p = weight_list[r]*math.exp(alpha)\n",
    "                    Z += p\n",
    "            print('Z',Z)\n",
    "            \n",
    "            for i in range(len(weight_list)): # update weight\n",
    "                if error_raws_index[i] == 1:\n",
    "                    weight_list[i] = weight_list[i]*math.exp(alpha)/Z\n",
    "                elif error_raws_index[i] == 0:\n",
    "                    weight_list[i] = weight_list[i]*math.exp(-alpha)/Z\n",
    "            \n",
    "#             print('%%',weight_list,len(weight_list))\n",
    "            tp = 0\n",
    "            for u in weight_list:\n",
    "                tp+=u\n",
    "#             print('%%',tp)\n",
    "             \n",
    "            a=0 # a is for calculating the interval\n",
    "            interval_weight_list = [0] # generate interval, for example weight [1/6,1/2,1/3] \n",
    "                                      # then interval [1/6,1/6+1/2,1/6+1/2+1/3]\n",
    "            sum = 0\n",
    "            while a <= len(weight_list)-1:\n",
    "                sum += weight_list[a]\n",
    "                interval_weight_list.append(sum)\n",
    "                a+=1\n",
    "            \n",
    "            new_train = pd.DataFrame(columns = self.train.columns.tolist())\n",
    "                                           # new_train is the training set that following the data weight w1,w2...\n",
    "            for j in range(len(random_number)):\n",
    "                true = False\n",
    "                for k in range(len(interval_weight_list)-1):\n",
    "                    if true==True:\n",
    "                        break\n",
    "                    left_side = interval_weight_list[k]\n",
    "                    right_side = interval_weight_list[k+1]\n",
    "                    if (left_side<random_number[j]) and (random_number[j]<=right_side):\n",
    "                        row = self.train.iloc[k:k+1,:]\n",
    "#                         print('*****',row)\n",
    "                        new_train = new_train.append(row,ignore_index = True)\n",
    "                        true = True\n",
    "        \n",
    "            s = self.Gini_CART_DT(new_train,weight_list)\n",
    "            error, threshold, error_raws_index,column_name = \\\n",
    "                                                    s[0],s[1],s[2],s[3]\n",
    "            count += 1\n",
    "            print('-----')\n",
    "            \n",
    "        print('>>',final_classifier)\n",
    "        print('>>',error_list)\n",
    "        print('>>',column_names)\n",
    "        print('>>',count)\n",
    "        print('>>',pn)\n",
    "        self.count = count\n",
    "        self.error_list = error_list\n",
    "        return [final_classifier,error_list,column_names, count, pn]\n",
    "    \n",
    "    def Gx1(self,threshold,x): # G(x) = {+1,-1}\n",
    "        if x<=threshold:\n",
    "            return 1\n",
    "        elif x>threshold:\n",
    "            return -1\n",
    "        \n",
    "    def Gx2(self,threshold,x): # G(x) = {-1,+1}\n",
    "        if x<=threshold:\n",
    "            return -1\n",
    "        elif x>threshold:\n",
    "            return 1\n",
    "        \n",
    "    def predict(self,trainset,final_classifier,column_names,pn): # this is for getting the final results of G(x)\n",
    "#         a = self.get_final_classifier()\n",
    "#         final_classifier,error_list,column_names, count, pn= a[0],a[1],a[2],a[3],a[4]\n",
    "        predict_label = []\n",
    "        for j in range(trainset.shape[0]):\n",
    "            result = 0 # final result G(x) = sign[alpha1G1(x)+alpha2G2(x)+...]\n",
    "            for i in range(len(final_classifier)):\n",
    "#                 print(trainset.loc[:,column_names[i]].values[j])\n",
    "                if pn[i] == '+':\n",
    "                    result += final_classifier[i][0]*self.Gx1(final_classifier[i][1],trainset.\\\n",
    "                                                              loc[:,column_names[i]].\\\n",
    "                                                         values[j])\n",
    "                elif pn[i] == '-':\n",
    "                    result += final_classifier[i][0]*self.Gx2(final_classifier[i][1],trainset.\\\n",
    "                                                              loc[:,column_names[i]].\\\n",
    "                                                         values[j])\n",
    "#             print('result',result)\n",
    "            if result<=0:\n",
    "                predict_label.append(-1)\n",
    "            elif result >0:\n",
    "                predict_label.append(1)\n",
    "        return predict_label\n",
    "    \n",
    "    def accuracy(self,y,predict_label):\n",
    "        count = 0\n",
    "        for i in range(y.shape[0]):\n",
    "            if predict_label[i] == y.iloc[i:i+1,:].values[0][0]:\n",
    "                count+=1\n",
    "        accuracy = count/y.shape[0]\n",
    "        return accuracy\n",
    "    \n",
    "    def prt(self):\n",
    "        return [self.error_list,self.count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.10333333333333326\n",
      "Z 0.6087875016967901\n",
      "-----\n",
      "error: 0.11530159491545794\n",
      "Z 0.6387711237224474\n",
      "-----\n",
      "error: 0.14366185220096067\n",
      "Z 0.7014930489267881\n",
      "-----\n",
      "error: 0.08762076731747193\n",
      "Z 0.565485166745048\n",
      "-----\n",
      "error: 0.06534667679465883\n",
      "Z 0.4942731577844541\n",
      "-----\n",
      "error: 0.04420248430946111\n",
      "Z 0.4110894048273826\n",
      "-----\n",
      "error: 0.0826374974789099\n",
      "Z 0.5506670191298297\n",
      "-----\n",
      "error: 0.021992623028118163\n",
      "Z 0.29331858148069107\n",
      "-----\n",
      "error: 0.02317012612789813\n",
      "Z 0.30088716412047506\n",
      "-----\n",
      "error: 0.0370161051487884\n",
      "Z 0.37760250586245914\n",
      "-----\n",
      ">> [(1.0803620875583468, 0.0461), (1.018847770894128, 824.2), (0.8926015205502861, 712.1), (1.1715188446364244, 834.1), (1.3302345484257034, 0.16161), (1.5368825466731488, 0.11928), (1.2035195827670657, 0.2639), (1.8974050663495459, 0.17549), (1.8707243618600289, 34.86), (1.6293417977462452, 3.215)]\n",
      ">> [0.10333333333333326, 0.11530159491545794, 0.14366185220096067, 0.08762076731747193, 0.06534667679465883, 0.04420248430946111, 0.0826374974789099, 0.021992623028118163, 0.02317012612789813, 0.0370161051487884]\n",
      ">> [9, 25, 25, 25, 29, 8, 28, 29, 15, 14]\n",
      ">> 11\n",
      ">> ['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n",
      "time 2229.658966779709\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "trainer = Adaboost(train)\n",
    "f = trainer.get_final_classifier()\n",
    "final_classifier,error_list,column_names, count, pn = f[0],f[1],f[2],f[3],f[4]\n",
    "predict_label = trainer.predict(trainset=train_x,final_classifier=final_classifier,\\\n",
    "                                column_names=column_names,pn=pn)\n",
    "accuracy = trainer.accuracy(train_y,predict_label)\n",
    "time2 = time.time()\n",
    "print('time',time2-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8933333333333333\n",
      "predict [1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "train_error [0.10333333333333326, 0.11530159491545794, 0.14366185220096067, 0.08762076731747193, 0.06534667679465883, 0.04420248430946111, 0.0826374974789099, 0.021992623028118163, 0.02317012612789813, 0.0370161051487884]\n",
      "iteration: 11\n"
     ]
    }
   ],
   "source": [
    "a = trainer.prt()\n",
    "error_list, count= a[0],a[1]\n",
    "print('accuracy',accuracy)\n",
    "print('predict',predict_label)\n",
    "print('train_error',error_list)\n",
    "print('iteration:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy1 0.9107806691449815\n",
      "accuracy1 0.9107806691449815\n",
      "accuracy1 0.9144981412639405\n",
      "accuracy1 0.9368029739776952\n",
      "accuracy1 0.9368029739776952\n",
      "accuracy1 0.9628252788104089\n",
      "accuracy1 0.9591078066914498\n",
      "accuracy1 0.9628252788104089\n",
      "accuracy1 0.9814126394052045\n",
      "accuracy1 0.9739776951672863\n",
      "accuracy1 0.9739776951672863\n",
      "time 0.8851468563079834\n"
     ]
    }
   ],
   "source": [
    "time3 = time.time()\n",
    "\n",
    "final_classifier =  [(1.0803620875583468, 0.0461), (1.018847770894128, 824.2), (0.8926015205502861, 712.1),\\\n",
    "                     (1.1715188446364244, 834.1), (1.3302345484257034, 0.16161), (1.5368825466731488, 0.11928), \\\n",
    "                     (1.2035195827670657, 0.2639), (1.8974050663495459, 0.17549), (1.8707243618600289, 34.86),\\\n",
    "                     (1.6293417977462452, 3.215)]\n",
    "column_names = [9, 25, 25, 25, 29, 8, 28, 29, 15, 14]\n",
    "pn = ['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n",
    "i = 1\n",
    "accuracy1_list = []\n",
    "while i<=11:\n",
    "    final_classifier1 =  final_classifier[:i]\n",
    "    column_names1 = column_names[:i]\n",
    "    pn1 = pn[:i]\n",
    "    predict_label1 = trainer.predict(trainset=test_x,final_classifier=final_classifier1,\\\n",
    "                                    column_names=column_names1,pn=pn1)\n",
    "    accuracy1 = trainer.accuracy(test_y,predict_label1)\n",
    "    print('accuracy1',accuracy1)\n",
    "    accuracy1_list.append(accuracy1)\n",
    "    i+=1\n",
    "time4 = time.time()\n",
    "print('time',time4-time3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_list = [1-accuracy1_list[i] for i in range(len(accuracy1_list))]\n",
    "test_error_list\n",
    "error_list+=[0.0043]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-- control the iteration and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i for i in range(1,12)], error_list)\n",
    "plt.plot([i for i in range(1,12)], test_error_list)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-- Train SVM model and get accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction report without scale:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.91      0.94       203\n",
      "           1       0.77      0.94      0.84        66\n",
      "\n",
      "    accuracy                           0.91       269\n",
      "   macro avg       0.87      0.92      0.89       269\n",
      "weighted avg       0.93      0.91      0.92       269\n",
      "\n",
      "Prediction report with scale:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.98      0.99       203\n",
      "           1       0.93      1.00      0.96        66\n",
      "\n",
      "    accuracy                           0.98       269\n",
      "   macro avg       0.96      0.99      0.98       269\n",
      "weighted avg       0.98      0.98      0.98       269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# build a svc without scaling the data\n",
    "\n",
    "clf = SVC(C=1.0, \n",
    "          kernel='rbf',\n",
    "          gamma='scale', \n",
    "          tol=0.001, \n",
    "          class_weight=None, \n",
    "          max_iter=-1)\n",
    "\n",
    "clf_without_scale = SVC(C=1.0, \n",
    "                      kernel='rbf',\n",
    "                      gamma='scale', \n",
    "                      tol=0.001, \n",
    "                      class_weight=None, \n",
    "                      max_iter=-1)\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "                     train_x.to_numpy(), test_x.to_numpy(), train_y.to_numpy().ravel(), test_y.to_numpy().ravel()\n",
    "\n",
    "# First scale the data, then build svc\n",
    "steps = [('scaler', StandardScaler()), ('svm', clf)]\n",
    "clf_with_scale = Pipeline(steps)\n",
    "\n",
    "# training svc using training dataset\n",
    "clf_without_scale.fit(X_train, y_train)\n",
    "clf_with_scale.fit(X_train, y_train)\n",
    "\n",
    "# predict test dataset\n",
    "y_pred_without_scale = clf_without_scale.predict(X_test)\n",
    "y_pred_with_scale = clf_with_scale.predict(X_test)\n",
    "print(\"Prediction report without scale:\")\n",
    "print(classification_report(y_test, y_pred_without_scale))\n",
    "print(\"Prediction report with scale:\")\n",
    "print(classification_report(y_test, y_pred_with_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('scaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('svm',\n",
       "                                        SVC(C=1.0, break_ties=False,\n",
       "                                            cache_size=200, class_weight=None,\n",
       "                                            coef0=0.0,\n",
       "                                            decision_function_shape='ovr',\n",
       "                                            degree=3, gamma='scale',\n",
       "                                            kernel='rbf', max_iter=-1,\n",
       "                                            probability=False,\n",
       "                                            random_state=None, shrinking=True,\n",
       "                                            tol=0.001, verbose=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'svm__C': [0.01, 0.1, 1, 10, 100, 1000],\n",
       "                         'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100, 1000,\n",
       "                                        'scale', 'auto']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# split training data and testing data with a ratio of 7:3\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "                      train_x.to_numpy(), test_x.to_numpy(), train_y.to_numpy().ravel(), test_y.to_numpy().ravel()\n",
    "\n",
    "# build the svm\n",
    "clf = SVC(C=1.0, \n",
    "          kernel='rbf',\n",
    "          gamma='scale', \n",
    "          tol=0.001, \n",
    "          class_weight=None, \n",
    "          max_iter=-1)\n",
    "\n",
    "# build the pipeline\n",
    "steps = [('scaler', StandardScaler()), ('svm', clf)]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# perform a grid search\n",
    "parameters = {'svm__C': [0.01, 0.1, 1, 10, 100, 1000], 'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 'scale', 'auto']}\n",
    "gs_clf = GridSearchCV(pipeline, parameters, cv=5)\n",
    "gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svm__C': 1, 'svm__gamma': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.98      0.99       203\n",
      "           1       0.94      1.00      0.97        66\n",
      "\n",
      "    accuracy                           0.99       269\n",
      "   macro avg       0.97      0.99      0.98       269\n",
      "weighted avg       0.99      0.99      0.99       269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test, gs_clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
